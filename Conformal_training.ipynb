{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f55356",
   "metadata": {},
   "source": [
    "## Basic importing and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "253d0a8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchcp.classification.scores import THR, APS, SAPS, RAPS\n",
    "from torchcp.classification.predictors import SplitPredictor, ClusterPredictor, ClassWisePredictor\n",
    "from torchcp.classification.loss import ConfTr\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e34e284",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# random seed\n",
    "SEED = 1 \n",
    "NUM_CLASS = 10\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 30\n",
    "EVAL_INTERVAL=1\n",
    "SAVE_DIR = './log'\n",
    "\n",
    "# Optimizer\n",
    "LEARNING_RATE = 1e-3\n",
    "MOMENTUM = 0.9\n",
    "STEP=5\n",
    "GAMMA=0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03b903a",
   "metadata": {},
   "source": [
    "## Choice of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f50794",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Change the dset num from 1 to 6 to chose the dataset\n",
    "1: STL10\n",
    "2: CIFAR10\n",
    "3: CIFAR100\n",
    "4: Stanford Dogs\n",
    "5: FasthionMINIST\n",
    "6: UTKFace\n",
    "\"\"\"\n",
    "dset = 1 # change this num in 1,2,3,4,5,6 to chose dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1cd2eb",
   "metadata": {},
   "source": [
    "#### 1. STL10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd587e48",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Total number of samples in the STL10: 5000\n"
     ]
    }
   ],
   "source": [
    "if dset == 1:\n",
    "    ## STL10\n",
    "    transform_cifar10_train = transforms.Compose([\n",
    "        transforms.Resize(size=128),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_cifar10_test = transforms.Compose([\n",
    "        # Add a resize operation to match training size\n",
    "        transforms.Resize(size=32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    train_set = torchvision.datasets.STL10(root='/data/lab/data', split='train',\n",
    "                                           download=True, transform=transform_cifar10_train)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                                             shuffle=True, num_workers=2,pin_memory=True)\n",
    "    test_set = torchvision.datasets.STL10(root='/data/lab/data', split='test',\n",
    "                                           download=True, transform=transform_cifar10_test)\n",
    "    cal_dataset, test_dataset = torch.utils.data.random_split(test_set, [4000, 4000])\n",
    "    # test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "    #                                          shuffle=False, num_workers=2,pin_memory=True)\n",
    "    cal_data_loader = torch.utils.data.DataLoader(cal_dataset, batch_size=1600, shuffle=False, pin_memory=True)\n",
    "    test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1600, shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "    class_names = ['airplane', 'bird', 'car', 'cat', 'deer', 'dog', 'horse', 'monkey', 'ship', 'truck']\n",
    "\n",
    "\n",
    "    dataset_name = 'STL10'\n",
    "\n",
    "    data_loader_size = len(train_dataloader.dataset)\n",
    "    print(f\"Total number of samples in the STL10: {data_loader_size}\")\n",
    "\n",
    "    class_less = True # mark the amount of class number(less or equal than 10 is marked True)\n",
    "\n",
    "    # Preparing a calibration data and a test data.\n",
    "    train_data_loader = train_dataloader\n",
    "    # cal_data_loader = torch.utils.data.DataLoader(train_set, batch_size=1600, shuffle=False, pin_memory=True)\n",
    "    # test_data_loader = torch.utils.data.DataLoader(test_set, batch_size=1600, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b164e6d",
   "metadata": {},
   "source": [
    "#### 2. CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff1f329",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if dset == 2:\n",
    "    # CIFAR10\n",
    "    transform_cifar10_test = transforms.Compose([\n",
    "        transforms.Resize(size=32),\n",
    "        transforms.CenterCrop(size=(32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    test_set = torchvision.datasets.CIFAR10(root='/data/lab/data', train=False,\n",
    "                                           download=True, transform=transform_cifar10_test)\n",
    "    train_set = torchvision.datasets.CIFAR10(root='/data/lab/data', train=True,\n",
    "                                           download=True, transform=transform_cifar10_test)\n",
    "\n",
    "    # resize test and train dataset\n",
    "    test_set = Subset(test_set, range(10000))##10000\n",
    "    train_set = Subset(train_set, range(10000))##10000\n",
    "    # #######################\n",
    "\n",
    "\n",
    "    # test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "    #                                          shuffle=True, num_workers=2)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                                             shuffle=True, num_workers=2,pin_memory=True)\n",
    "    # set calibration and test dataloader\n",
    "    cal_dataset, test_dataset = torch.utils.data.random_split(test_set, [5000, 5000])\n",
    "    cal_data_loader = torch.utils.data.DataLoader(cal_dataset, batch_size=1600, shuffle=False, pin_memory=True)\n",
    "    test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1600, shuffle=False, pin_memory=True)\n",
    "\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    dataset_name = 'CIFAR10'\n",
    "\n",
    "    data_loader_size = len(test_data_loader.dataset)\n",
    "    print(f\"Total number of samples in the CIFAR10: {data_loader_size}\")\n",
    "\n",
    "    class_less = True # mark the amount of class number(less or equal than 10 is marked True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f2825c",
   "metadata": {},
   "source": [
    "#### 3. CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "966de53e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if dset == 3:\n",
    "    ## CIFAR100\n",
    "    transform_cifar100_test = transforms.Compose([\n",
    "        transforms.Resize(size=32),\n",
    "        transforms.CenterCrop(size=(32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    test_set = torchvision.datasets.CIFAR100(root='/data/lab/data', train=False,\n",
    "                                           download=True, transform=transform_cifar100_test)\n",
    "    train_set = torchvision.datasets.CIFAR100(root='/data/lab/data', train=True,\n",
    "                                           download=True, transform=transform_cifar100_test)\n",
    "\n",
    "    # resize test and train dataset\n",
    "    test_set = Subset(test_set, range(10000))##10000\n",
    "    train_set = Subset(train_set, range(10000))##10000\n",
    "    # #######################\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                                             shuffle=True, num_workers=2)\n",
    "    # set calibration and test dataloader\n",
    "    cal_dataset, test_dataset = torch.utils.data.random_split(test_set, [5000, 5000])\n",
    "    cal_data_loader = torch.utils.data.DataLoader(cal_dataset, batch_size=1600, shuffle=False, pin_memory=True)\n",
    "    test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1600, shuffle=False, pin_memory=True)\n",
    "\n",
    "    class_names = [\n",
    "        'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle',\n",
    "        'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle',\n",
    "        'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
    "        'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard',\n",
    "        'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain',\n",
    "        'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree',\n",
    "        'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
    "        'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar',\n",
    "        'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
    "        'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
    "    ]\n",
    "\n",
    "    dataset_name = 'CIFAR100'\n",
    "\n",
    "    data_loader_size = len(test_data_loader.dataset)\n",
    "    print(f\"Total number of samples in the CIFAR100: {data_loader_size}\")\n",
    "\n",
    "    class_less = False # mark the amount of class number(less or equal than 10 is marked True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae216785",
   "metadata": {},
   "source": [
    "#### 4. Stanford Dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71def61a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if dset == 4:\n",
    "    ##stanford dogs\n",
    "    size = 128\n",
    "    transform_stanford_dogs_test = transforms.Compose([\n",
    "        transforms.Resize(size=size),\n",
    "        transforms.CenterCrop(size=size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Replace 'root_directory' with the path to the directory containing the Stanford Dogs dataset\n",
    "    test_set = datasets.ImageFolder(root='/data/lab/data/Images', transform=transform_stanford_dogs_test)\n",
    "    class_names = test_set.classes\n",
    "\n",
    "    test_set = Subset(test_set, range(20000))##10000\n",
    "\n",
    "    cal_dataset, test_dataset, train_set = torch.utils.data.random_split(test_set, [5000, 5000,10000])\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                                             shuffle=True, num_workers=2)\n",
    "    cal_data_loader = torch.utils.data.DataLoader(cal_dataset, batch_size=256, shuffle=True, pin_memory=True)\n",
    "    test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=True, pin_memory=True)\n",
    "\n",
    "\n",
    "    # List of class names for the Stanford Dogs dataset (based on the breed names)\n",
    "\n",
    "    dataset_name = 'Stanford Dogs'\n",
    "\n",
    "    data_loader_size = len(test_data_loader.dataset)\n",
    "    print(f\"Total number of samples in the Stanford Dogs dataset: {data_loader_size}\")\n",
    "\n",
    "    class_less = False # mark the amount of class number(less or equal than 10 is marked True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d5e9e",
   "metadata": {},
   "source": [
    "#### 5. Fashionminist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de865481",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if dset == 5:\n",
    "    transform_fashionmnist_test = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize(size=128),\n",
    "        transforms.CenterCrop(size=(128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # Use FashionMNIST mean and std\n",
    "    ])\n",
    "\n",
    "    test_set = torchvision.datasets.FashionMNIST(root='/data/lab/data', \n",
    "                                                 download=True, \n",
    "                                                 train=False,  # Set train=False for the test set\n",
    "                                                 transform=transform_fashionmnist_test)\n",
    "    test_set = Subset(test_set, range(10000))##5000\n",
    "    train_set = torchvision.datasets.FashionMNIST(root='/data/lab/data', train=True,\n",
    "                                           download=True, transform=transform_fashionmnist_test)\n",
    "    train_set = Subset(train_set, range(10000))##5000\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                                             shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "    cal_dataset, test_dataset = torch.utils.data.random_split(test_set, [5000, 5000])\n",
    "    cal_data_loader = torch.utils.data.DataLoader(cal_dataset, batch_size=256, shuffle=False, pin_memory=True)\n",
    "    test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False, pin_memory=True)\n",
    "\n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "    dataset_name = 'FashionMNIST'\n",
    "\n",
    "    data_loader_size = len(test_data_loader.dataset)\n",
    "    print(f\"Total number of samples in the FashionMNIST test set: {data_loader_size}\")\n",
    "\n",
    "    class_less = len(class_names) <= 10  # Check if the number of classes is 10 or less\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1290e1c9",
   "metadata": {},
   "source": [
    "#### 6. UTKFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85b35730",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if dset == 6:    \n",
    "    ##UTKFace for RESNET18\n",
    "    BATCH_SIZE = 256\n",
    "    size = 128\n",
    "    transform_stanford_dogs = transforms.Compose([\n",
    "        transforms.Resize(size=size),\n",
    "        transforms.CenterCrop(size=size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    root_directory = '/data/lab/data/UTKFace_age'\n",
    "    full_dataset = datasets.ImageFolder(root=root_directory, transform=transform_stanford_dogs)\n",
    "\n",
    "    # Define the ratio of train and test split\n",
    "    train_ratio = 0.6\n",
    "    test_ratio = 0.2\n",
    "    cal_ratio = 0.2\n",
    "\n",
    "    # Calculate the sizes of train and test sets\n",
    "    train_size = int(train_ratio * len(full_dataset))\n",
    "    test_size = int(test_ratio * len(full_dataset))\n",
    "    cal_size = len(full_dataset) - train_size - test_size\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    train_set, test_set,cal_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size,cal_size])\n",
    "\n",
    "    # Create DataLoaders for train and test sets\n",
    "    train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    test_data_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    cal_data_loader = DataLoader(cal_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "    # List of class names for the Stanford Dogs dataset (based on the breed names)\n",
    "    class_names = full_dataset.classes\n",
    "\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    dataset_name = 'UTKFace_age'\n",
    "\n",
    "    print(num_classes)\n",
    "    # data_loader_size = len(test_dataloader.dataset)\n",
    "    # print(f\"Total number of samples in the UTKFace dataset: {data_loader_size}\")\n",
    "\n",
    "    class_less = False # mark the amount of class number(less or equal than 10 is marked True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d613ccb6",
   "metadata": {},
   "source": [
    "## Model defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a2f13da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'ResNet18'\n",
    "def train(criterion):\n",
    "        # resnet18\n",
    "        model = torchvision.models.resnet18(weights=\"IMAGENET1K_V1\", progress=True)\n",
    "        num_ftrs = model.fc.in_features\n",
    "\n",
    "        # 替换模型最后的全连接层\n",
    "        model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "        # 将模型移至GPU（如果可用）\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = model.to(device)\n",
    "\n",
    "        # 定义损失函数和优化器\n",
    "        criterion = criterion\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "        # 训练模型\n",
    "        for epoch in range(5):  # 训练10个epoch\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(train_dataloader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                if i % 100 == 99:    # 每100个batch输出一次损失\n",
    "                    print(f'Epoch [{epoch + 1}, {i + 1}], Loss: {running_loss / 100:.4f}')\n",
    "                    running_loss = 0.0\n",
    "            print(f'Epoch [{epoch + 1}, {i + 1}], Loss: {running_loss / 100:.4f}')\n",
    "\n",
    "\n",
    "        print('Finished Training')\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e40ebd",
   "metadata": {},
   "source": [
    "## Conformal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a837a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment--Data : STL10, Model : ResNet18, Alpha : 0.1\n",
      "The size of calibration set is 4000.\n",
      "Epoch [1, 40], Loss: 0.6641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchcp/classification/predictors/split.py:67: UserWarning: The value of quantile exceeds 1. It should be a value in (0,1). To avoid program crash, the threshold is set as torch.inf.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2, 40], Loss: 0.2558\n",
      "Epoch [3, 40], Loss: 0.1873\n",
      "Epoch [4, 40], Loss: 0.1517\n",
      "Epoch [5, 40], Loss: 0.1356\n",
      "Finished Training\n",
      "0: {'Coverage_rate': 0.89575, 'Average_size': 6.099}\n",
      "Epoch [1, 40], Loss: 0.6601\n",
      "Epoch [2, 40], Loss: 0.2607\n",
      "Epoch [3, 40], Loss: 0.1877\n",
      "Epoch [4, 40], Loss: 0.1547\n",
      "Epoch [5, 40], Loss: 0.1281\n",
      "Finished Training\n",
      "1: {'Coverage_rate': 0.892, 'Average_size': 6.44675}\n",
      "Epoch [1, 40], Loss: 0.9196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchcp/classification/predictors/split.py:61: UserWarning: The number of scores is 0, which is a invalid scores. To avoid program crash, the threshold is set as torch.inf.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2, 40], Loss: 0.5132\n",
      "Epoch [3, 40], Loss: 0.4511\n",
      "Epoch [4, 40], Loss: 0.4199\n",
      "Epoch [5, 40], Loss: 0.4015\n",
      "Finished Training\n",
      "2: {'Coverage_rate': 0.903, 'Average_size': 4.74325}\n",
      "\n",
      "\n",
      "Epoch [1, 40], Loss: 0.6871\n",
      "Epoch [2, 40], Loss: 0.2586\n",
      "Epoch [3, 40], Loss: 0.1912\n",
      "Epoch [4, 40], Loss: 0.1643\n",
      "Epoch [5, 40], Loss: 0.1489\n",
      "Finished Training\n",
      "3: {'Coverage_rate': 0.89575, 'Average_size': 5.70325}\n",
      "Epoch [1, 40], Loss: 0.6642\n",
      "Epoch [2, 40], Loss: 0.2540\n",
      "Epoch [3, 40], Loss: 0.1939\n",
      "Epoch [4, 40], Loss: 0.1655\n",
      "Epoch [5, 40], Loss: 0.1450\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "weight = 0.2 ## from 0.02 to 0.6\n",
    "penalty = 0.001 ## from 0.001 to 0.01\n",
    "\n",
    "# print basic settings\n",
    "print(f\"Experiment--Data : {dataset_name}, Model : {model_name}, Alpha : {alpha}\")\n",
    "print(f\"The size of calibration set is {len(cal_dataset)}.\")\n",
    "\n",
    "# write outcomes to a txt\n",
    "with open('Wide_Compare.txt', 'a') as file:\n",
    "            file.write(f\"--------------------------------\\n\")\n",
    "            file.write(f\"Dataset: {dataset_name}\\n\")\n",
    "            file.write(f\"Model: {model_name}\\n\")\n",
    "            file.write(f\"Size of classes: {len(class_names)}\\n\")\n",
    "            file.write(f\"SAPS weight: {weight}\\n\")\n",
    "            file.write(f\"PAPS penalty: {penalty}\\n\")\n",
    "            \n",
    "t = 0 # recorder of score and predictor\n",
    "\n",
    "# Options of score function: THR, APS, SAPS, RAPS\n",
    "# Define a conformal prediction algorithm. Optional: SplitPredictor, ClusterPredictor, ClassWisePredictor\n",
    "for score in [THR(),APS(),SAPS(weight = weight),RAPS(penalty = penalty)]:\n",
    "    for predictor in [SplitPredictor\n",
    "                      ,ClusterPredictor\n",
    "                      ,ClassWisePredictor]:\n",
    "        #define predictor and criterion\n",
    "        predictor1 = predictor(score_function=score)\n",
    "        criterion = ConfTr(weight=0.01,\n",
    "                        predictor=predictor1,\n",
    "                        alpha=0.05,\n",
    "                        fraction=0.5,\n",
    "                        loss_type=\"valid\",\n",
    "                        base_loss_fn=nn.CrossEntropyLoss())\n",
    "        ####### train #######\n",
    "        model = train(criterion)\n",
    "        ####### Calibrate prediction #######\n",
    "        predictor = predictor(score_function=score,model = model)\n",
    "        \n",
    "        # Calibrating the predictor with significance level as 0.1\n",
    "        predictor.calibrate(cal_data_loader, alpha)\n",
    "        evaluate = predictor.evaluate(test_data_loader)\n",
    "        \n",
    "        # write outcomes to a txt\n",
    "        with open('Wide_Compare.txt', 'a') as file:\n",
    "            file.write(f\"--------------{t}--------------\\n\")\n",
    "            file.write(f\"Coverage_rate: {evaluate['Coverage_rate']}\\n\")\n",
    "            file.write(f\"Average_size: {evaluate['Average_size']}\\n\")\n",
    "        \n",
    "        # print outcomes\n",
    "        print(f'{t}: {evaluate}')\n",
    "        if t%3 == 2:\n",
    "            print('\\n')\n",
    "        t+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
